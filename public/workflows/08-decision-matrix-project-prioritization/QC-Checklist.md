# Quality Control Checklist - Decision Matrix for Project Prioritization

## Pre-Scoring Preparation

### 1. Decision Context Clarity
- [ ] Decision statement is written in a single, clear sentence
- [ ] Decision type is identified (selection, sequencing, resource allocation, go/no-go)
- [ ] Time horizon is defined (quarter, year, multi-year)
- [ ] Constraints are explicitly stated (budget, headcount, time, capacity)
- [ ] Number of projects to select or rank is specified
- [ ] Decision-makers and their authority levels are identified
- [ ] Stakeholders who will be affected are listed
- [ ] Ground rules for the scoring exercise are documented

### 2. Project List Completeness
- [ ] All candidate projects are listed with unique identifiers
- [ ] Each project has a clear, concise name (no ambiguous titles)
- [ ] Each project has a one-to-two sentence description
- [ ] Project sponsors or champions are identified
- [ ] Estimated effort or cost is noted for each project (even rough estimates)
- [ ] Current status is documented (new idea, in progress, stalled)
- [ ] Duplicate or overlapping projects have been merged or clarified
- [ ] No viable candidates have been excluded without documented reason
- [ ] List size is manageable (3-20 projects per scoring session)

### 3. Criteria Selection Quality
- [ ] Between 4 and 7 evaluation criteria are selected
- [ ] Criteria are independent (not measuring the same thing twice)
- [ ] At least one cost/effort criterion is included to balance benefit criteria
- [ ] At least one criterion is directly tied to strategic goals or OKRs
- [ ] Each criterion is clearly defined with a written description
- [ ] Criteria differentiate between the specific projects being evaluated
- [ ] No critical evaluation dimension has been omitted
- [ ] Criteria are phrased consistently (all positive or clear direction of "better")

### 4. Weighting Integrity
- [ ] Weights are assigned to all criteria and sum to 100%
- [ ] Weights were determined before any project scoring began
- [ ] Weights reflect actual organizational or strategic priorities
- [ ] No single criterion exceeds 40% weight (unless strongly justified)
- [ ] No criterion has less than 5% weight (if it matters that little, remove it)
- [ ] Weight rationale is documented for each criterion
- [ ] Weights were validated with stakeholders before scoring
- [ ] Weights are not reverse-engineered to favor a predetermined outcome

### 5. Scoring Scale and Anchors
- [ ] A consistent scoring scale is defined (1-5 or 1-10)
- [ ] Written scoring anchors exist for each criterion at each level
- [ ] Anchors are specific enough to reduce interpretation differences
- [ ] The scale direction is clear (e.g., for "Effort," is 5 = high effort or low effort?)
- [ ] Anchor definitions have been shared with all raters
- [ ] Example scores are provided for at least one criterion to calibrate raters
- [ ] The scale provides sufficient granularity to differentiate projects

## Scoring Quality Control

### 6. Individual Scoring Discipline
- [ ] Each rater scored independently before group discussion
- [ ] Scoring was done one criterion at a time across all projects (not one project at a time)
- [ ] All cells in the matrix are populated (no blanks)
- [ ] Scores fall within the defined scale range (no out-of-bounds values)
- [ ] Scores vary meaningfully (not all projects scored identically on a criterion)
- [ ] Low-confidence scores are flagged for further discussion
- [ ] Each score has at least a brief rationale or evidence note
- [ ] Raters did not share scores with each other before completing individual scoring

### 7. Bias Detection and Mitigation
- [ ] Anchoring bias checked: scores are not clustered around the first project scored
- [ ] Champion bias checked: project sponsors did not inflate their own project scores
- [ ] Recency bias checked: recently discussed projects are not systematically overscored
- [ ] Halo effect checked: a strong score on one criterion did not inflate others
- [ ] Groupthink checked: team scores show genuine variance, not artificial consensus
- [ ] Status quo bias checked: currently running projects are not automatically scored higher
- [ ] Framing bias checked: project descriptions use neutral language
- [ ] Similar projects have similar scores (consistency check)

### 8. Team Calibration (if applicable)
- [ ] Individual scores were revealed and compared after independent scoring
- [ ] Discrepancies of 2+ points (on a 5-point scale) were discussed
- [ ] Discrepancy discussions focused on evidence and interpretation, not persuasion
- [ ] Adjusted scores reflect new information, not social pressure
- [ ] Minority opinions were heard and documented even if not adopted
- [ ] Final calibrated scores were agreed upon by all raters
- [ ] Calibration changes are documented with reasoning

## Calculation and Ranking Verification

### 9. Formula and Calculation Accuracy
- [ ] Weighted Score = Raw Score x Criterion Weight (verified for all cells)
- [ ] Total Weighted Score = Sum of all Weighted Scores per project (verified)
- [ ] Ranking is based on Total Weighted Score in descending order
- [ ] Spot-check performed: manually calculated 2-3 cells to verify formulas
- [ ] No circular references or broken formulas in the spreadsheet
- [ ] Rounding is applied consistently (if at all)
- [ ] Tied scores have a documented tiebreaker rule
- [ ] Aggregation method for multiple raters is correctly applied (average, sum, etc.)

### 10. Ranking Reasonableness
- [ ] Top-ranked project passes the "gut check" (does it make strategic sense?)
- [ ] Bottom-ranked project makes sense as lowest priority
- [ ] No obviously misranked projects (e.g., a clearly urgent project ranked last)
- [ ] Natural score breakpoints are identified for tier assignments
- [ ] Tier assignments are logical (Tier 1/2/3 or High/Medium/Low)
- [ ] Close-ranked projects (within 5% of each other) are flagged for scrutiny
- [ ] Rankings are consistent with known strategic priorities
- [ ] Counter-intuitive rankings are explained with specific criteria-level evidence

### 11. Sensitivity Analysis Completeness
- [ ] At least 2-3 alternative weight scenarios were tested
- [ ] Uncertain scores were varied by +/- 1 point to test stability
- [ ] Projects that remain top-ranked across scenarios are identified ("safe bets")
- [ ] Projects whose ranking is weight-sensitive are flagged ("swing projects")
- [ ] At least one scenario tests the primary constraint changing (e.g., budget cut)
- [ ] Sensitivity analysis results are documented and included in rationale
- [ ] No ranking decision relies on a single uncertain score

## Documentation and Communication

### 12. Decision Rationale Quality
- [ ] Executive summary captures: decision question, selected projects, methodology
- [ ] Per-project rationale explains why each project landed in its tier
- [ ] Tier 1 rationale highlights specific strengths (not just "highest score")
- [ ] Tier 2 rationale identifies what would elevate projects to Tier 1
- [ ] Tier 3 rationale explains deprioritization without dismissiveness
- [ ] Assumptions made during scoring are listed explicitly
- [ ] Constraints that influenced the outcome are documented
- [ ] Re-evaluation triggers are defined (what would change this decision?)

### 13. Visualization and Presentation
- [ ] Matrix table is formatted clearly with aligned columns and readable fonts
- [ ] Color coding distinguishes tiers or score ranges at a glance
- [ ] At least one chart or visual accompanies the data (bar chart, 2x2 matrix, or radar)
- [ ] Visual representations match the underlying data accurately
- [ ] Summary view fits on one page or slide for executive presentation
- [ ] Detailed matrix is available as backup for questions
- [ ] Charts have clear labels, titles, and legends
- [ ] Visual aids do not distort or misrepresent the scoring

### 14. Stakeholder Communication
- [ ] Results were shared with all decision-makers and contributors
- [ ] Methodology was explained transparently (not just results)
- [ ] Questions and challenges were invited and addressed
- [ ] Feedback was documented and incorporated where appropriate
- [ ] Final approved list has explicit sign-off from decision authority
- [ ] Decision communication was sent to all affected stakeholders
- [ ] Dissenting opinions are noted in the record
- [ ] Next steps and kick-off timelines are communicated for selected projects

## Ongoing Maintenance

### 15. Reusability and Archival
- [ ] Matrix template is saved in a shared, accessible location
- [ ] Template is version-labeled (v1.0, v1.1, etc.)
- [ ] Criteria library is maintained for different decision types
- [ ] Past scoring sessions are archived with dates and participants
- [ ] Review cadence is scheduled (monthly check-in, quarterly re-score)
- [ ] Update triggers are defined and documented
- [ ] Process feedback has been collected from participants
- [ ] Improvements are noted for the next scoring cycle

### 16. Ethical and Fairness Review
- [ ] Criteria do not systematically disadvantage any team or department
- [ ] Scoring process gave equal voice to all participants
- [ ] No project was prejudged or pre-scored outside the formal process
- [ ] The matrix was not manipulated to justify a predetermined outcome
- [ ] Transparency: all stakeholders can see the criteria, weights, and scores
- [ ] Objectivity: scores are based on evidence, not personal preference alone
- [ ] Accountability: decision-makers are identified and have signed off

### 17. Integration with Downstream Processes
- [ ] Selected projects have assigned owners
- [ ] Resource allocation reflects the priority ranking
- [ ] Project kick-off timelines are established for Tier 1 projects
- [ ] Deferred projects (Tier 2/3) have clear conditions for re-evaluation
- [ ] The priority list is integrated with portfolio management or roadmap tools
- [ ] Budget allocation matches the prioritization output
- [ ] Risk mitigations are identified for top-priority projects (link to Workflow #9)

## Quality Gates

**DO NOT FINALIZE the prioritization if:**
- Criteria weights were not set before scoring began
- Any rater did not complete independent scoring
- More than 20% of scores lack evidence or rationale
- Sensitivity analysis was not performed for close-ranked projects
- A stakeholder with decision authority has not reviewed the results
- The ranking contradicts known strategic mandates without explanation

**ESCALATE for additional review if:**
- Two or more raters disagree by 3+ points on any single score
- The top-ranked project changes depending on which weight scenario is used
- A previously committed or mandatory project scored below Tier 1
- Budget allocation required exceeds available resources even for Tier 1 only
- Cross-departmental projects show significant scoring bias by department

## Checklist Completion

**Date of Review:** _________________

**Reviewer:** _________________

**Total Items Checked:** _____ / 95+

**Pass Threshold:** 90% (85+ items checked)

**Status:** [ ] PASSED - Prioritization complete and defensible  |  [ ] NEEDS REMEDIATION

**Critical Findings (if any):**
_______________________________________________________________________________
_______________________________________________________________________________

**Remediation Plan:**
_______________________________________________________________________________
_______________________________________________________________________________

**Next Review Date:** _________________
